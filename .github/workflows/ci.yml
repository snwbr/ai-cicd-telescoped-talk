name: AI CI â€” ML Test Selection + LLM Diagnosis (Modular)

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  ARTIFACT_DIR: files
  PYTHONUNBUFFERED: "1"
  PY_VERSION: "3.11"
  VENV_DIR: ".venv"

# Reusable cache key para el venv
# (mismo en todos los jobs => cache compartido)
defaults:
  run:
    shell: bash

jobs:
  prepare-env:
    name: Prepare env & cache venv
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PY_VERSION }}

      - name: Cache venv
        id: cache-venv
        uses: actions/cache@v4
        with:
          path: ${{ env.VENV_DIR }}
          key: venv-${{ runner.os }}-${{ env.PY_VERSION }}-${{ hashFiles('requirements.txt') }}

      - name: Create venv & install deps (if cache miss)
        if: steps.cache-venv.outputs.cache-hit != 'true'
        run: |
          python -m venv $VENV_DIR
          $VENV_DIR/bin/python -m pip install --upgrade pip
          $VENV_DIR/bin/pip install -r requirements.txt

      - name: Prepare artifact dir
        run: mkdir -p $ARTIFACT_DIR

      - name: Set CI diff env (push/PR compatibility)
        run: |
          echo "GITHUB_EVENT_BEFORE=${{ github.event.before }}" >> $GITHUB_ENV
          echo "GITHUB_BASE_REF=${{ github.base_ref }}" >> $GITHUB_ENV
          echo "GITHUB_EVENT_NAME=${{ github.event_name }}" >> $GITHUB_ENV

  train-model:
    name: Train model
    needs: [prepare-env]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }
      - name: Restore venv
        uses: actions/cache@v4
        with:
          path: ${{ env.VENV_DIR }}
          key: venv-${{ runner.os }}-${{ env.PY_VERSION }}-${{ hashFiles('requirements.txt') }}
      - run: mkdir -p $ARTIFACT_DIR
      - name: Train
        run: $VENV_DIR/bin/python ml/train_test_selector.py
      - name: Upload ML artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ml-model
          path: |
            ${{ env.ARTIFACT_DIR }}/model_rf.pkl
            ${{ env.ARTIFACT_DIR }}/test_index.json

  collect-changes:
    name: Collect changed files
    needs: [prepare-env]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }
      - name: Restore venv
        uses: actions/cache@v4
        with:
          path: ${{ env.VENV_DIR }}
          key: venv-${{ runner.os }}-${{ env.PY_VERSION }}-${{ hashFiles('requirements.txt') }}
      - run: mkdir -p $ARTIFACT_DIR
      - name: Collect
        run: $VENV_DIR/bin/python ci/collect_changed_files.py
      - name: Upload changed files
        uses: actions/upload-artifact@v4
        with:
          name: changed
          path: ${{ env.ARTIFACT_DIR }}/changed_files.json

  predict-tests:
    name: Predict tests (ML)
    needs: [train-model, collect-changes]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }
      - name: Restore venv
        uses: actions/cache@v4
        with:
          path: ${{ env.VENV_DIR }}
          key: venv-${{ runner.os }}-${{ env.PY_VERSION }}-${{ hashFiles('requirements.txt') }}
      - run: mkdir -p $ARTIFACT_DIR
      - name: Download ML model
        uses: actions/download-artifact@v4
        with:
          name: ml-model
          path: ${{ env.ARTIFACT_DIR }}
      - name: Download changed files
        uses: actions/download-artifact@v4
        with:
          name: changed
          path: ${{ env.ARTIFACT_DIR }}
      - name: Predict
        env:
          # Ajusta a gusto:
          # TOP_K: "2"
          # PROB_THRESHOLD: "0.30"
          MIN_TESTS: "1"
        run: $VENV_DIR/bin/python ml/predict_tests.py
      - name: Upload selection
        uses: actions/upload-artifact@v4
        with:
          name: selection
          path: ${{ env.ARTIFACT_DIR }}/selected_tests.json

  run-tests:
    name: Run selected tests
    needs: [predict-tests]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }
      - name: Restore venv
        uses: actions/cache@v4
        with:
          path: ${{ env.VENV_DIR }}
          key: venv-${{ runner.os }}-${{ env.PY_VERSION }}-${{ hashFiles('requirements.txt') }}
      - run: mkdir -p $ARTIFACT_DIR
      - name: Download selection
        uses: actions/download-artifact@v4
        with:
          name: selection
          path: ${{ env.ARTIFACT_DIR }}
      - name: Run pytest (continue on error to allow diagnosis)
        id: pytest
        continue-on-error: true
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: $VENV_DIR/bin/python ci/run_selected_tests.py
      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            ${{ env.ARTIFACT_DIR }}/report.xml
            ${{ env.ARTIFACT_DIR }}/pytest_output.log

  diagnose:
    name: Diagnose failures (LLM)
    needs: [run-tests]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }
      - name: Restore venv
        uses: actions/cache@v4
        with:
          path: ${{ env.VENV_DIR }}
          key: venv-${{ runner.os }}-${{ env.PY_VERSION }}-${{ hashFiles('requirements.txt') }}
      - run: mkdir -p $ARTIFACT_DIR
      - name: Download test results
        uses: actions/download-artifact@v4
        with:
          name: test-results
          path: ${{ env.ARTIFACT_DIR }}
      - name: Download selection
        uses: actions/download-artifact@v4
        with:
          name: selection
          path: ${{ env.ARTIFACT_DIR }}
      - name: Diagnose
        id: diagnose
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: $VENV_DIR/bin/python ci/diagnose_failure_llm.py
      - name: Upload diagnosis
        uses: actions/upload-artifact@v4
        with:
          name: diagnosis
          path: ${{ env.ARTIFACT_DIR }}/diagnosis.txt

  summarize-and-gate:
    name: Summary & final gate
    needs: [run-tests, diagnose]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }
      - run: mkdir -p $ARTIFACT_DIR

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: test-results
          path: ${{ env.ARTIFACT_DIR }}
      - uses: actions/download-artifact@v4
        with:
          name: diagnosis
          path: ${{ env.ARTIFACT_DIR }}

      - name: Append AI diagnosis to Summary
        if: always()
        run: |
          echo "# When automation is not enough: smart CI/CD pipelines" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "## Commit & Event" >> "$GITHUB_STEP_SUMMARY"
          echo "- Commit: $GITHUB_SHA" >> "$GITHUB_STEP_SUMMARY"
          echo "- Event:  $GITHUB_EVENT_NAME" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "## AI Diagnosis" >> "$GITHUB_STEP_SUMMARY"
          if [ -s $ARTIFACT_DIR/diagnosis.txt ]; then
            echo "" >> "$GITHUB_STEP_SUMMARY"
            echo '```' >> "$GITHUB_STEP_SUMMARY"
            sed -e 's/\r$//' $ARTIFACT_DIR/diagnosis.txt >> "$GITHUB_STEP_SUMMARY"
            echo '```' >> "$GITHUB_STEP_SUMMARY"
          else
            echo "_No diagnosis available (no failures or no log)_" >> "$GITHUB_STEP_SUMMARY"
          fi

      # Gate final: parse el JUnit y falla si hubo errores/fallos
      - name: Fail workflow if tests failed
        run: |
          python - << 'PY'
          import os, sys, xml.etree.ElementTree as ET
          p = os.path.join(os.environ.get("ARTIFACT_DIR","files"), "report.xml")
          if not os.path.exists(p):
              print("No report.xml found; assuming no tests -> pass")
              sys.exit(0)
          root = ET.parse(p).getroot()
          fails = errs = tests = 0
          if root.tag == "testsuite":
              tests  = int(root.attrib.get("tests","0"))
              fails  = int(root.attrib.get("failures","0"))
              errs   = int(root.attrib.get("errors","0"))
          else:
              for ts in root.iter("testsuite"):
                  tests += int(ts.attrib.get("tests","0"))
                  fails += int(ts.attrib.get("failures","0"))
                  errs  += int(ts.attrib.get("errors","0"))
          print(f"JUnit totals: tests={tests} failures={fails} errors={errs}")
          sys.exit(1 if (fails>0 or errs>0) else 0)
          PY
